{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup \n",
    "import openai as openai #Extracting content metadata\n",
    "import fitz #pdf reading library\n",
    "import time #to ensure we don't call too often from openai\n",
    "from bs4 import BeautifulSoup #to extract XML info -> will be eliminated eventually\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Library to import pre-trained model for sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Calculate similarities between sentences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# package for finding local minimas\n",
    "from scipy.signal import argrelextrema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is partially a dummy function. This extraction is limited by the fact that XML isn't standard. \n",
    "However, I want to start parsing and iterating over txt using GPT as a way of mechanizing/beginning to evaluate our \n",
    "thoughts on how to gain greater info about these papers. \n",
    "\"\"\"\n",
    "def splitXMLParagraphs(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Passing the stored data inside\n",
    "    # the beautifulsoup parser, storing\n",
    "    # the returned object\n",
    "    Bs_data = BeautifulSoup(data, \"xml\")\n",
    "    para = Bs_data.find_all('p')\n",
    "    paragraphs = []\n",
    "\n",
    "    for x in range(len(para)): \n",
    "        if len(para[x].text) < 2800:\n",
    "            paragraphs.append(para[x].text)\n",
    "        else: \n",
    "            para[x.text].split\n",
    "            x -= 1\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'format': 'PDF 1.4',\n",
       " 'title': 'doi:10.1016/j.snb.2008.10.030',\n",
       " 'author': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': 'Elsevier',\n",
       " 'producer': 'Acrobat Distiller 7.0 (Windows)',\n",
       " 'creationDate': 'D:20090112130006Z',\n",
       " 'modDate': \"D:20090117135818+05'30'\",\n",
       " 'trapped': '',\n",
       " 'encryption': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pdfMetadata(filepath): \n",
    "    doc = fitz.open(filepath)\n",
    "    metadata = doc.metadata\n",
    "    return metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfTextExtraction(filename, filepath): \n",
    "    doc = fitz.open(filepath)  # open document\n",
    "    with open(filepath, 'r') as out:\n",
    "        for page in doc:  # iterate the document pages\n",
    "            text = page.get_text().encode(\"utf8\")  # get plain text (is in UTF-8)\n",
    "            out.write(text)  # write text of page\n",
    "            out.write(bytes((12,)))  # write page delimiter (form feed 0x0C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a bunch of code from This guy -> https://medium.com/@npolovinkin/how-to-chunk-text-into-paragraphs-using-python-8ae66be38ea6\n",
    "def rev_sigmoid(x:float)->float:\n",
    "    return (1 / (1 + math.exp(0.5*x)))\n",
    "    \n",
    "def activate_similarities(similarities:np.array, p_size=10)->np.array:\n",
    "    \"\"\" Function returns list of weighted sums of activated sentence similarities\n",
    "\n",
    "    Args:\n",
    "        similarities (numpy array): it should square matrix where each sentence corresponds to another with cosine similarity\n",
    "        p_size (int): number of sentences are used to calculate weighted sum \n",
    "\n",
    "    Returns:\n",
    "        list: list of weighted sums\n",
    "    \"\"\"\n",
    "    # To create weights for sigmoid function we first have to create space. P_size will determine number of sentences used and the size of weights vector.\n",
    "    x = np.linspace(-10,10,p_size)\n",
    "    # Then we need to apply activation function to the created space\n",
    "    y = np.vectorize(rev_sigmoid) \n",
    "        # Because we only apply activation to p_size number of sentences we have to add zeros to neglect the effect of every additional sentence and to match the length ofvector we will multiply\n",
    "    activation_weights = np.pad(y(x),(0,similarities.shape[0]-p_size))\n",
    "    ### 1. Take each diagonal to the right of the main diagonal\n",
    "    diagonals = [similarities.diagonal(each) for each in range(0,similarities.shape[0])]\n",
    "    ### 2. Pad each diagonal by zeros at the end. Because each diagonal is different length we should pad it with zeros at the end\n",
    "    diagonals = [np.pad(each, (0,similarities.shape[0]-len(each))) for each in diagonals]\n",
    "    ### 3. Stack those diagonals into new matrix\n",
    "    diagonals = np.stack(diagonals)\n",
    "    ### 4. Apply activation weights to each row. Multiply similarities with our activation.\n",
    "    diagonals = diagonals * activation_weights.reshape(-1,1)\n",
    "    ### 5. Calculate the weighted sum of activated similarities\n",
    "    activated_similarities = np.sum(diagonals, axis=0)\n",
    "    return activated_similarities\n",
    "  \n",
    "\n",
    "def CreateModularContent(path, fname, sentencetransformer):\n",
    "    \"\"\" Function returns a list of paragraphs from a pdf\n",
    "\n",
    "    Args:\n",
    "        path (string): the file path to the PDF in concern\n",
    "        fname (string): file name of the pdf\n",
    "        sentencetransformer (sentencetransformer instance): Takes an instance of the sentence transformer library\n",
    "\n",
    "    Returns:\n",
    "        paragraphs: list of paragraphs in the file\n",
    "    \"\"\"\n",
    "    #reading the desired file\n",
    "    with open(path+fname + \".txt\", 'r') as file:\n",
    "     contents = file.read()\n",
    "\n",
    "    #separating the file into an array based on when there are periods. \n",
    "    list_of_contents = contents.split(\".\")\n",
    "    embeddings = sentencetransformer.encode(list_of_contents)\n",
    "\n",
    "        \n",
    "    # Create similarities matrix\n",
    "    similarities = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Lets apply activated_similarities. For long sentences i reccomend to use 10 or more sentences (not sure what p_size does)\n",
    "    activated_similarities = activate_similarities(similarities, p_size=similarities.shape[0])\n",
    "\n",
    "    ### 6. Find relative minima of our vector. For all local minimas and save them to variable with argrelextrema function\n",
    "    minmimas = argrelextrema(activated_similarities, np.less, order=2) #order parameter controls how frequent should be splits. I would not reccomend changing this parameter.\n",
    "    # plot the flow of our text with activated similarities\n",
    "\n",
    "    #visualization stuff that we don't need \n",
    "    # lets create empty fig for our plor\n",
    "    #fig, ax = plt.subplots()\n",
    "    #sns.lineplot(y=activated_similarities, x=range(len(activated_similarities)), ax=ax).set_title('Relative minimas');\n",
    "    # Now lets plot vertical lines in order to see where we created the split\n",
    "    #plt.vlines(x=minmimas, ymin=min(activated_similarities), ymax=max(activated_similarities), colors='purple', ls='--', lw=1, label='vline_multiple - full height')\n",
    "\n",
    "    #Get the order number of the sentences which are in splitting points\n",
    "    split_points = [each for each in minmimas[0]]\n",
    "    # Create empty string\n",
    "    text = ''\n",
    "    for num,each in enumerate(list_of_contents):\n",
    "        # Check if sentence is a minima (splitting point)\n",
    "        if num in split_points:\n",
    "            # If it is than add a dot to the end of the sentence and a paragraph before it.\n",
    "            text+=f'\\n{each}. '\n",
    "        else:\n",
    "            # If it is a normal sentence just add a dot to the end and keep adding sentences.\n",
    "            text+=f'{each}. '\n",
    "   \n",
    "    with open(path + fname + \"_para\" + \".txt\", 'w') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    paragraphs = f.readline()\n",
    "    return paragraphs   \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limitation -> recipe for paragraph extraction, but not necessarily basic metadata like authors etc\n",
    "def contentMetadataRecipe(openai, paragraphs, prompt): \n",
    "# Imports GPT3 model. Using davinci at the moment for final outputs. Curie for testing. \n",
    "    counter = 0\n",
    "    res = \"\"\n",
    "    ans = []\n",
    "    #Wondering if we can retrieve the model earlier on -> so we don't have to do this multiple times. \n",
    "    #openai.Model.retrieve(\"text-curie-001\")\n",
    "    openai.Model.retrieve(\"text-curie-001\")\n",
    "    limcurie = 2040\n",
    "    limdavinci = 2040\n",
    "    \n",
    "    \n",
    "    # structures the base prompt for the model\n",
    "    #TO BE UPDATED. I want to train my own version of this. \n",
    "    #base_prompt = \"Paragraph:So yeah, do you see in those ecosystems really cool as pop in? Lots of cool projects, many more I forgot a bunch, but yeah, Jocelyn is always curating this cool landscape, so just check it out. I have the Twitter right there. And yeah, so we just heard about it. So sharing scientific data is super important. Why? Because, well, if we share data, we can collaborate much more easily. We can build bigger data sets and bigger data sets means more statistical power, reliable results, right? So that's pretty cool. And it also means more access to the data that, so there's not the same access to cool instruments that help you with data collection across labs. So if you're in an underfunded research institution, you just may not have the ability to collect the same type of data that a well-funded institution may have. So if we all share data, we all have better access to make cool scientific discoveries. So that's pretty cool, right? But also sharing scientific data right now. It's pretty expensive, it's pretty vulnerable because it's stored on centralized databases where we just have to trust that they keep the database running. It's also not rewarded. So currently, what counts in science is having your PDF cited, but it doesn't matter if you make your data accessible, like you just cannot accrue credit to it. Or there's some ways you can, but it's just not really easy. And it's also pretty painful. So there's a couple of repos out there where you can store your data. These are funded by some governmental institutions. There you access not great. And then also, if you want to find the data, you need to know which repo it's stored at. So you need to find the repo. Then you need to find the data. It's all, it's a hassle, so it's not great.\\nExample Summary:Sharing scientific data is important as it allows for better collaboration, bigger data sets, reliable results, and better access for researchers in underfunded institutions. However, currently sharing data is expensive, vulnerable, and not rewarded. It is stored on centralized databases which requires that we trust those servers to keep running. Also, there are no incentives for for making the data accessible. Currently, the only way that we can give credit for using someone else's work is citing their PDF. But with PDF citations, it doesn't matter if you make your data accessible. Sharing data right now isn't worth the cost and time for the researcher.\\nParagraph:\"\n",
    "\n",
    "    base_prompt = \"Does this paragraph describe the paper's \" + prompt + \"Answer with a Yes or No.\"\n",
    "    \n",
    "    for x in paragraphs: \n",
    "        thought = x.strip()\n",
    "        p = base_prompt + thought\n",
    "        \n",
    "        # Model parameters were determined through sandbox testing. Temp is fairly high to allow the model\n",
    "        response = openai.Completion.create(\n",
    "            #model = \"text-curie-001\",\n",
    "            model=\"text-curie-001\",\n",
    "            prompt = p,\n",
    "            max_tokens=400,\n",
    "            temperature=0.7,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0.5,\n",
    "            presence_penalty=0.5\n",
    "        )\n",
    "        answer = response[\"choices\"][0][\"text\"]\n",
    "        ans.append(answer)\n",
    "        \n",
    "        if answer.find(\"Yes\") != -1: \n",
    "            question = \"What is the paper's\" + prompt + \"?\" + thought\n",
    "            # Model parameters were determined through sandbox testing. Temp is fairly high to allow the model\n",
    "            response2 = openai.Completion.create(\n",
    "                #model = \"text-curie-001\",\n",
    "                model=\"text-curie-001\",\n",
    "                prompt = question,\n",
    "                max_tokens=400,\n",
    "                temperature=0.7,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0.5,\n",
    "                presence_penalty=0.5\n",
    "            )\n",
    "            res += response2[\"choices\"][0][\"text\"]    \n",
    "        \n",
    "        \n",
    "        counter+=1\n",
    "        #print(counter)\n",
    "        # A sleep counter because microsoft keeps limiting my creativity\n",
    "        if counter%30==0 and counter!=0:\n",
    "            print(\"\\n\\n\\nI am so sleepy\\n\\n\\n\")\n",
    "            time.sleep(60)\n",
    "        \n",
    "        final = \"Summarize these responses into one sentence that tells me the paper's\" + prompt + \"\\n\" + res\n",
    "\n",
    "        response3 = openai.Completion.create(\n",
    "                    #model = \"text-curie-001\",\n",
    "                    model=\"text-curie-001\",\n",
    "                    prompt = final,\n",
    "                    max_tokens=400,\n",
    "                    temperature=0.7,\n",
    "                    top_p=1,\n",
    "                    frequency_penalty=0.5,\n",
    "                    presence_penalty=0.5\n",
    "            )\n",
    "\n",
    "    return(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): \n",
    "    \"\"\"\n",
    "    A potential combo of the functions above to get a set of metadata out. \n",
    "    \"\"\"\n",
    "    \n",
    "    filepathpdf = \"/Users/desot1/Dev/desci/Zhuiykov_2009_Morphology of Pt-doped nanofabricated RuO2 sensing electrodes and their.pdf\"\n",
    "    \n",
    "    descriptiveMetadata = pdfMetadata(filepathpdf)\n",
    "\n",
    "    \n",
    "    contentMetadata = {}\n",
    "\n",
    "    categories = ['Research Question', 'Alterative Approaches', 'Hypothesis', 'Methodology', 'Results', 'Inferences']\n",
    "    \n",
    "    paragraphs = splitXMLParagraphs(filepathxml)\n",
    "\n",
    "    for i in categories: \n",
    "        contentMetadata[categories[i]].append(contentMetadataRecipe(paragraphs, categories[i])) \n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
